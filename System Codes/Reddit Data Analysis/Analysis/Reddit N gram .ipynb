{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import textblob as tb\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "from pprint import pprint\n",
    "import os, pickle\n",
    "import ast\n",
    "import datetime\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"reddit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4518"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates(\"id\")\n",
    "df3= df2.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>comment</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote</th>\n",
       "      <th>url</th>\n",
       "      <th>view_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7754</th>\n",
       "      <td>Charlie_Yu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['This new Infernal Affairs movie looks much w...</td>\n",
       "      <td>265</td>\n",
       "      <td>1.563868e+09</td>\n",
       "      <td>cgl20c</td>\n",
       "      <td>1800</td>\n",
       "      <td>2019-07-23 15:53:06</td>\n",
       "      <td>The white shirt man who shaked hands with Juni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/mqix492h0yb31.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7054</th>\n",
       "      <td>Leaf-Currency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['This is all kinds of amazing.', 'China fails...</td>\n",
       "      <td>194</td>\n",
       "      <td>1.570735e+09</td>\n",
       "      <td>dfwgk4</td>\n",
       "      <td>19828</td>\n",
       "      <td>2019-10-11 03:11:46</td>\n",
       "      <td>What a wild ride</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/ey71kfji5pr31.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>suprasa1yan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['What a son of a bitch.  And Carrie Lam still...</td>\n",
       "      <td>18</td>\n",
       "      <td>1.571737e+09</td>\n",
       "      <td>dlak6m</td>\n",
       "      <td>183</td>\n",
       "      <td>2019-10-22 17:32:02</td>\n",
       "      <td>HK Police swearing, instigating violence, call...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.youtube.com/watch?v=SvvkS1yTRwo&amp;fe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5270</th>\n",
       "      <td>Sporeboss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Twitter thread for this topic:\\n\\n\\nhttps://...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.571958e+09</td>\n",
       "      <td>dmhiym</td>\n",
       "      <td>122</td>\n",
       "      <td>2019-10-25 06:53:46</td>\n",
       "      <td>lab test result comparing both weapon use by h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/y4qyc5jx5iu31.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>ImANormalMan</td>\n",
       "      <td>Lets wish that they all win against the govern...</td>\n",
       "      <td>[\"I really hope you are right, and I hope that...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571936e+09</td>\n",
       "      <td>dmdkfw</td>\n",
       "      <td>24</td>\n",
       "      <td>2019-10-25 00:45:27</td>\n",
       "      <td>My wish for HongKong</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/HongKong/comments/dmd...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                               body  \\\n",
       "7754     Charlie_Yu                                                NaN   \n",
       "7054  Leaf-Currency                                                NaN   \n",
       "5534    suprasa1yan                                                NaN   \n",
       "5270      Sporeboss                                                NaN   \n",
       "5296   ImANormalMan  Lets wish that they all win against the govern...   \n",
       "\n",
       "                                                comment  comms_num  \\\n",
       "7754  ['This new Infernal Affairs movie looks much w...        265   \n",
       "7054  ['This is all kinds of amazing.', 'China fails...        194   \n",
       "5534  ['What a son of a bitch.  And Carrie Lam still...         18   \n",
       "5270  ['Twitter thread for this topic:\\n\\n\\nhttps://...          8   \n",
       "5296  [\"I really hope you are right, and I hope that...          1   \n",
       "\n",
       "           created      id  score            timestamp  \\\n",
       "7754  1.563868e+09  cgl20c   1800  2019-07-23 15:53:06   \n",
       "7054  1.570735e+09  dfwgk4  19828  2019-10-11 03:11:46   \n",
       "5534  1.571737e+09  dlak6m    183  2019-10-22 17:32:02   \n",
       "5270  1.571958e+09  dmhiym    122  2019-10-25 06:53:46   \n",
       "5296  1.571936e+09  dmdkfw     24  2019-10-25 00:45:27   \n",
       "\n",
       "                                                  title upvote  \\\n",
       "7754  The white shirt man who shaked hands with Juni...    NaN   \n",
       "7054                                   What a wild ride    NaN   \n",
       "5534  HK Police swearing, instigating violence, call...    NaN   \n",
       "5270  lab test result comparing both weapon use by h...    NaN   \n",
       "5296                               My wish for HongKong    NaN   \n",
       "\n",
       "                                                    url  view_num  \n",
       "7754                https://i.redd.it/mqix492h0yb31.jpg       NaN  \n",
       "7054                https://i.redd.it/ey71kfji5pr31.jpg       NaN  \n",
       "5534  https://www.youtube.com/watch?v=SvvkS1yTRwo&fe...       NaN  \n",
       "5270                https://i.redd.it/y4qyc5jx5iu31.jpg       NaN  \n",
       "5296  https://www.reddit.com/r/HongKong/comments/dmd...       NaN  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloblist = []\n",
    "i = 1\n",
    "for com in df3[[\"comment\"]].itertuples(index=False):\n",
    "    for i in com : \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            bloblist.append(tb(i))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeWordSet = set([\"don't\",\"never\", \"nothing\", \"nowhere\", \"noone\", \"none\", \"not\",\n",
    "                  \"hasn't\",\"hadn't\",\"can't\",\"couldn't\",\"shouldn't\",\"won't\",\n",
    "                  \"wouldn't\",\"don't\",\"doesn't\",\"didn't\",\"isn't\",\"aren't\",\"ain't\",\"wasn't\",\"weren't\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english')) - set(negativeWordSet) \n",
    "\n",
    "stop_words.add('chu')\n",
    "\n",
    "def tf_idf_process(blob):\n",
    "    '''\n",
    "    initially only .lower() & remove stopwords => find there are ♫, - etc non-alphabetic chr\n",
    "        so we use .isalpha() \n",
    "    then we found words like \"wk\", \"tm\" have very high tf-idf values\n",
    "        they are just initials of a person, no meaning\n",
    "        so we use word.words() to check\n",
    "    '''\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \n",
    "                \"N\": wordnet.NOUN, \n",
    "                \"V\": wordnet.VERB, \n",
    "                \"R\": wordnet.ADV}\n",
    "   \n",
    "    # lowercase + remove stopwords + remove non-alphabets + lemmatize \n",
    "    # english_words = set(words.words())\n",
    "      \n",
    "    processed_blob = [word.lower() for word in blob.words if word.isalpha()]\n",
    "    lemmatized_blob = []\n",
    "    for word, tag in nltk.pos_tag(processed_blob):\n",
    "        if tag[0] in tag_dict:\n",
    "            word = WordNetLemmatizer().lemmatize(word,tag_dict[tag[0]])\n",
    "        lemmatized_blob.append(word)\n",
    "    \n",
    "    string = \" \".join(lemmatized_blob)\n",
    "    return tb(string)\n",
    "\n",
    "def n_gram_top_phrases(n):\n",
    "\n",
    "    n_gram_freq_dict = {}\n",
    "    for blob in bloblist:\n",
    "        words = blob.words\n",
    "        for i in range(len(words)-(n-1)):\n",
    "            word_phrase = tuple([words[i+x] for x in range(n)])\n",
    "            in_stop_words = sum([word in stop_words for word in word_phrase]) != n\n",
    "            \n",
    "            if in_stop_words:\n",
    "            \n",
    "                if word_phrase in n_gram_freq_dict:\n",
    "                    n_gram_freq_dict[word_phrase] += 1\n",
    "                else:\n",
    "                    n_gram_freq_dict[word_phrase] = 1\n",
    "    #n_gram_freq_dict = clear_similar(n_gram_freq_dict)\n",
    "    sorted_n_gram = sorted(n_gram_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return n_gram_freq_dict\n",
    "\n",
    "def check_similar(a,b):\n",
    "    if len([x for x in a if x in b])>= len(a)-1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def combine_similar(a,b):\n",
    "    common = list(set(a) & set(b))\n",
    "    output =[]\n",
    "    if(len([x for x in a if x not in b]) >=1 or len([x for x in b if x not in a])>=1  ):\n",
    "        if list(a).index(common[0]) > list(b).index(common[0]):\n",
    "            output = list(a)\n",
    "            output.extend([x for x in list(b) if x not in set(a)])\n",
    "        else:\n",
    "            output = list(b)\n",
    "            output.extend([x for x in list(a) if x not in set(b)])\n",
    "\n",
    "    return tuple(output)\n",
    "\n",
    "def clear_similar(n_gram_freq_dict):\n",
    "    no_similar_dict ={}\n",
    "    for phrase1 in n_gram_freq_dict:\n",
    "        similar = False\n",
    "        for phrase2 in no_similar_dict:\n",
    "            if check_similar(phrase1,phrase2):\n",
    "                combined = combine_similar(phrase1,phrase2)\n",
    "                no_similar_dict[combined] = max(no_similar_dict[phrase2],n_gram_freq_dict[phrase1])+1   \n",
    "                del no_similar_dict[phrase2]\n",
    "                similar = True\n",
    "                break\n",
    "        if not similar:\n",
    "            no_similar_dict[phrase1] = n_gram_freq_dict[phrase1]\n",
    "            \n",
    "    return no_similar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "mafan= [\"seen\",\"'ve\",\"wan\",\"someone\",\"like\",\"look\",\"got\",\"would\",\"kong\",\"and\",\"hong\",\"'s\",'https',\"'s\",'gon','www.youtube.com/watch','n',\"s\",\"they\",\"see\",\"i\",\"in\",\"a\",\"lot\",\"to\",\"world\",\"the\",\"'m\",\"do\",\"i'\",\"think\",\"protestors\",\"much\",\"many\",\"go\",\"good\",\"still\",\"could\",\"'m\",\"'re\",\"n\\\\n\",\"also\",\"world\",\"this\",\"say\",\"going\",\"they\",\"i\",\"s\",\"not\",'’',\"n't\",\"onw\",\"the\",\"get\",'think',\"it\",\"even\",\"know\",\"see\",\"really\",'“','”']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd = list(stop_words) + mafan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    " trigram = n_gram_top_phrases(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_t = sorted(trigram.items(), key=lambda kv: kv[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'country', 'two') 37\n",
      "('Chinese', 'Communist', 'Party') 33\n",
      "('country', 'two', 'systems') 33\n",
      "('basic', 'human', 'right') 33\n",
      "('human', 'rights', 'abuses') 32\n",
      "('15', 'year', 'old') 28\n",
      "('www.reddit.com/message/compose', 'to=kittens_from_space', 'Exclude') 28\n",
      "('reddit.com/message/compose', 'to=WikiTextBot', 'message=Excludeme') 28\n",
      "('to=WikiTextBot', 'message=Excludeme', 'subject=Excludeme') 28\n",
      "('message=Excludeme', 'subject=Excludeme', 'Exclude') 28\n",
      "('np.reddit.com/r/HongKong/about/banned', 'FAQ', 'Information') 28\n",
      "('天安門', '天安门', '法輪功') 28\n",
      "('天安门', '法輪功', '李洪志') 28\n",
      "('Great', 'Leap', 'Forward') 26\n",
      "('basic', 'human', 'rights') 26\n",
      "('Tiananmen', 'Square', 'Massacre') 20\n",
      "('Nobel', 'Peace', 'Prize') 20\n",
      "('utm_source=share', 'utm_medium=ios_app', 'utm_name=iossmf') 20\n",
      "('Contact', '32', 'Developer') 19\n",
      "('32', '32', 'Support') 19\n",
      "('32', 'Support', '32') 19\n",
      "('www.paypal.me/synapsensalat', '32', '32') 19\n",
      "('32', '32', 'Github') 19\n",
      "('5Etfw', '7Ctwcamp', '5Etweetembed') 18\n",
      "('One', 'Country', 'Two') 18\n",
      "('long', 'time', 'ago') 18\n",
      "('Direct', 'soundless', 'link') 17\n",
      "('social', 'credit', 'score') 16\n",
      "(\"'ve\", 'never', 'seen') 16\n",
      "('social', 'credit', 'system') 16\n",
      "('year', 'old', 'girl') 16\n",
      "('tear', 'gas', 'canisters') 15\n",
      "('Country', 'Two', 'Systems') 15\n",
      "('E8', 'AD', 'A6') 15\n",
      "('Tiananmen', 'Square', 'protests') 15\n",
      "('32', '32', 'Contact') 15\n",
      "('32', 'Contact', '32') 15\n",
      "('bot', 'beep', 'boop') 14\n",
      "('beep', 'boop', 'Downvote') 14\n",
      "('www.reddit.com/message/compose', 'to=sneakpeekbot', 'Info') 14\n",
      "('moral', 'high', 'ground') 14\n",
      "('法輪功', '李洪志', 'Free') 14\n",
      "('李洪志', 'Free', 'Tibet') 14\n",
      "('Free', 'Tibet', '六四天安門事件') 14\n",
      "('Square', 'Massacre', '反右派鬥爭') 14\n",
      "('Anti-Rightist', 'Struggle', '大躍進政策') 14\n",
      "('Leap', 'Forward', '文化大革命') 14\n",
      "('Great', 'Proletarian', 'Cultural') 14\n",
      "('Proletarian', 'Cultural', 'Revolution') 14\n",
      "('Cultural', 'Revolution', '人權') 14\n",
      "('Revolution', '人權', 'Human') 14\n",
      "('人權', 'Human', 'Rights') 14\n",
      "('Human', 'Rights', '民運') 14\n",
      "('Rights', '民運', 'Democratization') 14\n",
      "('民運', 'Democratization', '自由') 14\n",
      "('Democratization', '自由', 'Freedom') 14\n",
      "('自由', 'Freedom', '獨立') 14\n",
      "('Freedom', '獨立', 'Independence') 14\n",
      "('獨立', 'Independence', '多黨制') 14\n",
      "('Independence', '多黨制', 'Multi-party') 14\n",
      "('多黨制', 'Multi-party', 'system') 14\n",
      "('Multi-party', 'system', '台灣') 14\n",
      "('system', '台灣', '臺灣') 14\n",
      "('台灣', '臺灣', 'Taiwan') 14\n",
      "('臺灣', 'Taiwan', 'Formosa') 14\n",
      "('Taiwan', 'Formosa', '中華民國') 14\n",
      "('Formosa', '中華民國', 'Republic') 14\n",
      "('China', '西藏', '土伯特') 14\n",
      "('西藏', '土伯特', '唐古特') 14\n",
      "('土伯特', '唐古特', 'Tibet') 14\n",
      "('唐古特', 'Tibet', '達賴喇嘛') 14\n",
      "('Tibet', '達賴喇嘛', 'Dalai') 14\n",
      "('達賴喇嘛', 'Dalai', 'Lama') 14\n",
      "('Dalai', 'Lama', '法輪功') 14\n",
      "('Lama', '法輪功', 'Falun') 14\n",
      "('法輪功', 'Falun', 'Dafa') 14\n",
      "('Falun', 'Dafa', '新疆維吾爾自治區') 14\n",
      "('Xinjiang', 'Uyghur', 'Autonomous') 14\n",
      "('Uyghur', 'Autonomous', 'Region') 14\n",
      "('Autonomous', 'Region', '諾貝爾和平獎') 14\n",
      "('Region', '諾貝爾和平獎', 'Nobel') 14\n",
      "('諾貝爾和平獎', 'Nobel', 'Peace') 14\n",
      "('Peace', 'Prize', '劉暁波') 14\n",
      "('Prize', '劉暁波', 'Liu') 14\n",
      "('劉暁波', 'Liu', 'Xiaobo') 14\n",
      "('Liu', 'Xiaobo', '民主') 14\n",
      "('Xiaobo', '民主', '言論') 14\n",
      "('民主', '言論', '思想') 14\n",
      "('言論', '思想', '反共') 14\n",
      "('思想', '反共', '反革命') 14\n",
      "('反共', '反革命', '抗議') 14\n",
      "('反革命', '抗議', '運動') 14\n",
      "('抗議', '運動', '騷亂') 14\n",
      "('運動', '騷亂', '暴亂') 14\n",
      "('騷亂', '暴亂', '騷擾') 14\n",
      "('暴亂', '騷擾', '擾亂') 14\n",
      "('騷擾', '擾亂', '抗暴') 14\n",
      "('擾亂', '抗暴', '平反') 14\n",
      "('抗暴', '平反', '維權') 14\n",
      "('平反', '維權', '示威游行') 14\n",
      "('維權', '示威游行', '李洪志') 14\n",
      "('示威游行', '李洪志', '法輪大法') 14\n",
      "('李洪志', '法輪大法', '大法弟子') 14\n",
      "('法輪大法', '大法弟子', '強制斷種') 14\n",
      "('大法弟子', '強制斷種', '強制堕胎') 14\n",
      "('強制斷種', '強制堕胎', '民族淨化') 14\n",
      "('強制堕胎', '民族淨化', '人體實驗') 14\n",
      "('民族淨化', '人體實驗', '肅清') 14\n",
      "('人體實驗', '肅清', '胡耀邦') 14\n",
      "('肅清', '胡耀邦', '趙紫陽') 14\n",
      "('胡耀邦', '趙紫陽', '魏京生') 14\n",
      "('趙紫陽', '魏京生', '王丹') 14\n",
      "('魏京生', '王丹', '還政於民') 14\n",
      "('王丹', '還政於民', '和平演變') 14\n",
      "('還政於民', '和平演變', '激流中國') 14\n",
      "('和平演變', '激流中國', '北京之春') 14\n",
      "('激流中國', '北京之春', '大紀元時報') 14\n",
      "('北京之春', '大紀元時報', '九評論共産黨') 14\n",
      "('大紀元時報', '九評論共産黨', '獨裁') 14\n",
      "('九評論共産黨', '獨裁', '專制') 14\n",
      "('獨裁', '專制', '壓制') 14\n",
      "('專制', '壓制', '統一') 14\n",
      "('壓制', '統一', '監視') 14\n",
      "('統一', '監視', '鎮壓') 14\n",
      "('監視', '鎮壓', '迫害') 14\n",
      "('鎮壓', '迫害', '侵略') 14\n",
      "('迫害', '侵略', '掠奪') 14\n",
      "('侵略', '掠奪', '破壞') 14\n",
      "('掠奪', '破壞', '拷問') 14\n",
      "('破壞', '拷問', '屠殺') 14\n",
      "('拷問', '屠殺', '活摘器官') 14\n",
      "('屠殺', '活摘器官', '誘拐') 14\n",
      "('活摘器官', '誘拐', '買賣人口') 14\n",
      "('誘拐', '買賣人口', '遊進') 14\n",
      "('買賣人口', '遊進', '走私') 14\n",
      "('遊進', '走私', '毒品') 14\n",
      "('走私', '毒品', '賣淫') 14\n",
      "('毒品', '賣淫', '春畫') 14\n",
      "('賣淫', '春畫', '賭博') 14\n",
      "('春畫', '賭博', '六合彩') 14\n",
      "('賭博', '六合彩', '天安門') 14\n",
      "('六合彩', '天安門', '天安门') 14\n",
      "('法輪功', '李洪志', 'Winnie') 14\n",
      "('2', 'million', 'people') 14\n",
      "(\"'Can\", 'someone', 'explain') 13\n",
      "('ref_src=twsrc', '5Etfw', '7Ctwcamp') 13\n",
      "('ref_url=https', '3A', '2F') 13\n",
      "('7Ctwcamp', '5Etweetembed', '7Ctwterm') 13\n",
      "('old.reddit.com/user/VredditDownloader/comments/cju1dg/info', '32', '32') 13\n",
      "('1', 'country', '2') 12\n",
      "('country', '2', 'systems') 12\n",
      "('E4', 'BA', 'BA') 12\n",
      "('30', 'years', 'ago') 12\n",
      "('redditsearch.io', 'term', 'dataviz=false') 12\n",
      "('term', 'dataviz=false', 'aggs=false') 12\n",
      "('searchtype=posts', 'comments', 'search=true') 12\n",
      "('comments', 'search=true', 'start=0') 12\n",
      "('provides', 'downloadable', 'video') 12\n",
      "('downloadable', 'video', 'links') 12\n",
      "('np.reddit.com/message/compose', 'to=/u/JohannesPertl', '32') 12\n",
      "('to=/u/JohannesPertl', '32', '32') 12\n",
      "('police', 'press', 'conference') 11\n",
      "('South', 'China', 'Sea') 11\n",
      "('Martin', 'Luther', 'King') 11\n",
      "('HK', 'police', 'force') 11\n",
      "('3A', '2F', '2Fwww.reddit.com') 11\n",
      "('2F', '2Fwww.reddit.com', '2Fr') 11\n",
      "('2Fwww.reddit.com', '2Fr', '2FHongKong') 11\n",
      "('2Fr', '2FHongKong', '2Fcomments') 11\n",
      "('behind', 'closed', 'doors') 11\n",
      "('10', 'year', 'old') 11\n",
      "('Tiananmen', 'Square', 'massacre') 11\n"
     ]
    }
   ],
   "source": [
    "for element in sorted_t:\n",
    "    i = element[0]\n",
    "    count = element[1]\n",
    "    \n",
    "    if i[0].lower() not in msd and i[1].lower() not in msd and i[2].lower() not in msd and count > 10:\n",
    "        \n",
    "        print(i,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = n_gram_top_phrases(2)\n",
    "sorted_b = sorted(bigram.items(), key=lambda kv: kv[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human rights 286\n",
      "Chinese government 220\n",
      "tear gas 196\n",
      "HK police 194\n",
      "police brutality 155\n",
      "police force 149\n",
      "Chinese people 140\n",
      "Carrie Lam 135\n",
      "universal suffrage 121\n",
      "extradition bill 118\n",
      "Tiananmen Square 114\n",
      "United States 113\n",
      "5 demands 103\n",
      "police officer 102\n",
      "years ago 101\n",
      "mainland China 100\n",
      "police officers 100\n",
      "year old 85\n",
      "first place 82\n",
      "Human Rights 82\n",
      "social media 76\n",
      "free speech 75\n",
      "trade war 71\n",
      "long time 69\n",
      "HK people 68\n",
      "peaceful protest 67\n",
      "fake news 65\n",
      "basic human 65\n",
      "North Korea 63\n",
      "police station 63\n",
      "riot police 61\n",
      "human right 61\n",
      "make sure 60\n",
      "HK government 60\n",
      "mainland Chinese 59\n",
      "one thing 58\n",
      "social credit 57\n",
      "Communist Party 57\n",
      "one day 56\n",
      "concentration camps 56\n",
      "every day 55\n",
      "innocent people 55\n",
      "3A 2F 55\n",
      "million people 55\n",
      "32 32 54\n",
      "one country 52\n",
      "metal rod 52\n",
      "five demands 51\n"
     ]
    }
   ],
   "source": [
    "for element in sorted_b:\n",
    "    i = element[0]\n",
    "    count = element[1]\n",
    "    \n",
    "    if i[0].lower() not in msd and i[1].lower() not in msd and count > 50:\n",
    "        kw = i[0] +\" \" + i[1]\n",
    "        print(kw,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
